{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XumW1PvfB-36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bO5nvE289ec.en.vtt']\n"
          ]
        }
      ],
      "source": [
        "# Put this in terminal\n",
        "# pip install transformers \n",
        "# pip install ipywidgets\n",
        "# pip install python-dotenv\n",
        " \n",
        "import subprocess #lets us download captions\n",
        "import os\n",
        "import re # Detect lines that are just numbers and skip them\n",
        "import glob #let's python search for files matching a pattern\n",
        "import time\n",
        "from transformers import pipeline\n",
        "\n",
        "files = glob.glob(\"*.vtt\")\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPiO_cZzCCqg"
      },
      "outputs": [],
      "source": [
        "video_url = \"https://www.youtube.com/watch?v=bO5nvE289ec\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "6JvLLfmVCk9r"
      },
      "outputs": [],
      "source": [
        "def get_transcript(video_url: str, lang = \"en\", cookies_file = \"cookies (1).txt\") -> str:\n",
        "  # First, get information about available captions\n",
        "  info_result = subprocess.run([\n",
        "      \"yt-dlp\",\n",
        "      \"--list-subs\",\n",
        "      video_url\n",
        "  ], check=False, text=True, capture_output=True)\n",
        "\n",
        "  print(\"Available captions:\")\n",
        "  print(info_result.stdout)\n",
        "  print(info_result.stderr)\n",
        "\n",
        "  match = re.search(r\"v=([a-zA-z0-9_-]+)\",video_url)\n",
        "  if not match:\n",
        "      raise ValueError(\"Invalid YouTube URL\")\n",
        "  else:\n",
        "    video_id = match.group(1)\n",
        "  \n",
        "  # This is a list so we have to get the first element which is our file\n",
        "  caption_file = glob.glob(f\"{video_id}.*\")\n",
        "  # If we already have the caption file, read it and return the transcript\n",
        "  if not caption_file:\n",
        "    print(\"No creator captions, trying auto captions\")\n",
        "    other_result = subprocess.run([\n",
        "    \"yt-dlp\", #download captions\n",
        "    \"--write-auto-sub\", #auto subtitles if possible\n",
        "    \"--write-sub\", #Also can use creator captions if they have it\n",
        "    \"--sub-langs\", f\"{lang}\", #english language\n",
        "    \"--skip-download\", #prevents actual video from being downloaded\n",
        "    \"--output\", \"%(id)s.%(ext)s\", #forces file names to be VIDEOID.ext\n",
        "    \"--verbose\", # Add verbose flag for debugging\n",
        "    video_url #enter actual link\n",
        "    ], check = False, text = True, capture_output = True)\n",
        "    caption_file = glob.glob(f\"{video_id}.vtt\")\n",
        "    if not caption_file:\n",
        "      raise ValueError(\"No captions found\")\n",
        "\n",
        "  file = caption_file[0]\n",
        "# Get rid of empty spaces and arrows\n",
        "  print(f\"Using caption file {file}\")\n",
        "  transcript = \"\"\n",
        "  with open(file, \"r\", encoding = \"utf-8\") as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      if not line:\n",
        "        continue\n",
        "      if \"-->\" in line:\n",
        "        continue\n",
        "      if re.match(r\"^\\d+$\", line):\n",
        "        continue\n",
        "      transcript += line + \" \"\n",
        "    cleaned_transcript = re.sub(r\"<.*?>\", \" \", transcript)\n",
        "  return cleaned_transcript.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "3RX4ilwdVJts",
        "outputId": "5d8f13b6-bcf9-451a-8a54-928bfd492b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available captions:\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=bO5nvE289ec\n",
            "[youtube] bO5nvE289ec: Downloading webpage\n",
            "[youtube] bO5nvE289ec: Downloading tv client config\n",
            "[youtube] bO5nvE289ec: Downloading tv player API JSON\n",
            "[youtube] bO5nvE289ec: Downloading web safari player API JSON\n",
            "[youtube] bO5nvE289ec: Downloading m3u8 information\n",
            "[info] Available automatic captions for bO5nvE289ec:\n",
            "Language      Name                                               Formats\n",
            "en-US                                                            vtt\n",
            "ab            Abkhazian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "aa            Afar                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "af            Afrikaans                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ak            Akan                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sq            Albanian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "am            Amharic                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ar            Arabic                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hy            Armenian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "as            Assamese                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ay            Aymara                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "az            Azerbaijani                                        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bn            Bangla                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ba            Bashkir                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eu            Basque                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "be            Belarusian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bho           Bhojpuri                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bs            Bosnian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "br            Breton                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bg            Bulgarian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "my            Burmese                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ca            Catalan                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ceb           Cebuano                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hans       Chinese (Simplified)                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hant       Chinese (Traditional)                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "co            Corsican                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hr            Croatian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cs            Czech                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "da            Danish                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dv            Divehi                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nl            Dutch                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dz            Dzongkha                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "en-orig       English (Original)                                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "en            English                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eo            Esperanto                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "et            Estonian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ee            Ewe                                                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fo            Faroese                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fj            Fijian                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fil           Filipino                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fi            Finnish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fr            French                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gaa           Ga                                                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gl            Galician                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lg            Ganda                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ka            Georgian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "de            German                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "el            Greek                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gn            Guarani                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gu            Gujarati                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ht            Haitian Creole                                     vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ha            Hausa                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "haw           Hawaiian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iw            Hebrew                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hi            Hindi                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hmn           Hmong                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hu            Hungarian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "is            Icelandic                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ig            Igbo                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "id            Indonesian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iu            Inuktitut                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ga            Irish                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "it            Italian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ja            Japanese                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "jv            Javanese                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kl            Kalaallisut                                        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kn            Kannada                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kk            Kazakh                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kha           Khasi                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "km            Khmer                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rw            Kinyarwanda                                        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ko            Korean                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kri           Krio                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ku            Kurdish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ky            Kyrgyz                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lo            Lao                                                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "la            Latin                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lv            Latvian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ln            Lingala                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lt            Lithuanian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lua           Luba-Lulua                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "luo           Luo                                                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lb            Luxembourgish                                      vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mk            Macedonian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mg            Malagasy                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ms            Malay                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ml            Malayalam                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mt            Maltese                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gv            Manx                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mi            Māori                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mr            Marathi                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mn            Mongolian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mfe           Morisyen                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ne            Nepali                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "new           Newari                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nso           Northern Sotho                                     vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "no            Norwegian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ny            Nyanja                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "oc            Occitan                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "or            Odia                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "om            Oromo                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "os            Ossetic                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pam           Pampanga                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ps            Pashto                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fa            Persian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pl            Polish                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt            Portuguese                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt-PT         Portuguese (Portugal)                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pa            Punjabi                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "qu            Quechua                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ro            Romanian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rn            Rundi                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ru            Russian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sm            Samoan                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sg            Sango                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sa            Sanskrit                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gd            Scottish Gaelic                                    vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sr            Serbian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "crs           Seselwa Creole French                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sn            Shona                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sd            Sindhi                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "si            Sinhala                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sk            Slovak                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sl            Slovenian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "so            Somali                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "st            Southern Sotho                                     vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "es            Spanish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "su            Sundanese                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sw            Swahili                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ss            Swati                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sv            Swedish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tg            Tajik                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ta            Tamil                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tt            Tatar                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "te            Telugu                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "th            Thai                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bo            Tibetan                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ti            Tigrinya                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "to            Tongan                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ts            Tsonga                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tn            Tswana                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tum           Tumbuka                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tr            Turkish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tk            Turkmen                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uk            Ukrainian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ur            Urdu                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ug            Uyghur                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uz            Uzbek                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ve            Venda                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "vi            Vietnamese                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "war           Waray                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cy            Welsh                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fy            Western Frisian                                    vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "wo            Wolof                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "xh            Xhosa                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yi            Yiddish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yo            Yoruba                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zu            Zulu                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ab-en-US      Abkhazian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "aa-en-US      Afar from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "af-en-US      Afrikaans from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ak-en-US      Akan from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sq-en-US      Albanian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "am-en-US      Amharic from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ar-en-US      Arabic from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hy-en-US      Armenian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "as-en-US      Assamese from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ay-en-US      Aymara from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "az-en-US      Azerbaijani from English (United States)           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bn-en-US      Bangla from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ba-en-US      Bashkir from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eu-en-US      Basque from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "be-en-US      Belarusian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bho-en-US     Bhojpuri from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bs-en-US      Bosnian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "br-en-US      Breton from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bg-en-US      Bulgarian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "my-en-US      Burmese from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ca-en-US      Catalan from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ceb-en-US     Cebuano from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hans-en-US Chinese (Simplified) from English (United States)  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hant-en-US Chinese (Traditional) from English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "co-en-US      Corsican from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hr-en-US      Croatian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cs-en-US      Czech from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "da-en-US      Danish from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dv-en-US      Divehi from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nl-en-US      Dutch from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dz-en-US      Dzongkha from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "en-en-US      English from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eo-en-US      Esperanto from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "et-en-US      Estonian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ee-en-US      Ewe from English (United States)                   vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fo-en-US      Faroese from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fj-en-US      Fijian from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fil-en-US     Filipino from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fi-en-US      Finnish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fr-en-US      French from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gaa-en-US     Ga from English (United States)                    vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gl-en-US      Galician from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lg-en-US      Ganda from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ka-en-US      Georgian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "de-en-US      German from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "el-en-US      Greek from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gn-en-US      Guarani from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gu-en-US      Gujarati from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ht-en-US      Haitian Creole from English (United States)        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ha-en-US      Hausa from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "haw-en-US     Hawaiian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iw-en-US      Hebrew from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hi-en-US      Hindi from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hmn-en-US     Hmong from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hu-en-US      Hungarian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "is-en-US      Icelandic from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ig-en-US      Igbo from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "id-en-US      Indonesian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iu-en-US      Inuktitut from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ga-en-US      Irish from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "it-en-US      Italian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ja-en-US      Japanese from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "jv-en-US      Javanese from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kl-en-US      Kalaallisut from English (United States)           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kn-en-US      Kannada from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kk-en-US      Kazakh from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kha-en-US     Khasi from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "km-en-US      Khmer from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rw-en-US      Kinyarwanda from English (United States)           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ko-en-US      Korean from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kri-en-US     Krio from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ku-en-US      Kurdish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ky-en-US      Kyrgyz from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lo-en-US      Lao from English (United States)                   vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "la-en-US      Latin from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lv-en-US      Latvian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ln-en-US      Lingala from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lt-en-US      Lithuanian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lua-en-US     Luba-Lulua from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "luo-en-US     Luo from English (United States)                   vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lb-en-US      Luxembourgish from English (United States)         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mk-en-US      Macedonian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mg-en-US      Malagasy from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ms-en-US      Malay from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ml-en-US      Malayalam from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mt-en-US      Maltese from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gv-en-US      Manx from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mi-en-US      Māori from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mr-en-US      Marathi from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mn-en-US      Mongolian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mfe-en-US     Morisyen from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ne-en-US      Nepali from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "new-en-US     Newari from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nso-en-US     Northern Sotho from English (United States)        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "no-en-US      Norwegian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ny-en-US      Nyanja from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "oc-en-US      Occitan from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "or-en-US      Odia from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "om-en-US      Oromo from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "os-en-US      Ossetic from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pam-en-US     Pampanga from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ps-en-US      Pashto from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fa-en-US      Persian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pl-en-US      Polish from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt-en-US      Portuguese from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt-PT-en-US   Portuguese (Portugal) from English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pa-en-US      Punjabi from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "qu-en-US      Quechua from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ro-en-US      Romanian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rn-en-US      Rundi from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ru-en-US      Russian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sm-en-US      Samoan from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sg-en-US      Sango from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sa-en-US      Sanskrit from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gd-en-US      Scottish Gaelic from English (United States)       vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sr-en-US      Serbian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "crs-en-US     Seselwa Creole French from English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sn-en-US      Shona from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sd-en-US      Sindhi from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "si-en-US      Sinhala from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sk-en-US      Slovak from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sl-en-US      Slovenian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "so-en-US      Somali from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "st-en-US      Southern Sotho from English (United States)        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "es-en-US      Spanish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "su-en-US      Sundanese from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sw-en-US      Swahili from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ss-en-US      Swati from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sv-en-US      Swedish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tg-en-US      Tajik from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ta-en-US      Tamil from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tt-en-US      Tatar from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "te-en-US      Telugu from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "th-en-US      Thai from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bo-en-US      Tibetan from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ti-en-US      Tigrinya from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "to-en-US      Tongan from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ts-en-US      Tsonga from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tn-en-US      Tswana from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tum-en-US     Tumbuka from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tr-en-US      Turkish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tk-en-US      Turkmen from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uk-en-US      Ukrainian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ur-en-US      Urdu from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ug-en-US      Uyghur from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uz-en-US      Uzbek from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ve-en-US      Venda from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "vi-en-US      Vietnamese from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "war-en-US     Waray from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cy-en-US      Welsh from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fy-en-US      Western Frisian from English (United States)       vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "wo-en-US      Wolof from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "xh-en-US      Xhosa from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yi-en-US      Yiddish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yo-en-US      Yoruba from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zu-en-US      Zulu from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "[info] Available subtitles for bO5nvE289ec:\n",
            "Language Name                    Formats\n",
            "en-US    English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "\n",
            "\n",
            "Using caption file bO5nvE289ec.en.vtt\n",
            "['WEBVTT', 'Kind:', 'captions', 'Language:', 'en', 'This', 'video', 'is', 'about', 'how', 'to', 'find', 'good', 'This', 'video', 'is', 'about', 'how', 'to', 'find', 'good', 'This', 'video', 'is', 'about', 'how', 'to', 'find', 'good', 'parameters', 'for', 'a', 'machine', 'learning', 'model.', 'parameters', 'for', 'a', 'machine', 'learning', 'model.', 'parameters', 'for', 'a', 'machine', 'learning', 'model.', 'The', 'search', 'for', 'good', 'parameters', 'is', 'known', 'The', 'search', 'for', 'good', 'parameters', 'is', 'known', 'The', 'search', 'for', 'good', 'parameters', 'is', 'known', 'as', 'optimization', 'and', 'the', 'tool', 'we', 'use', 'is', 'as', 'optimization', 'and', 'the', 'tool', 'we', 'use', 'is', 'as', 'optimization', 'and', 'the', 'tool', 'we', 'use', 'is', 'known', 'as', 'an', 'optimizer.', 'For', 'a', 'long', 'time,']\n"
          ]
        }
      ],
      "source": [
        "text = get_transcript(video_url)\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5eGXZylbPzT",
        "outputId": "324924e3-fef6-4e59-ebde-ccce7bee3a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This video is about how to find good parameters for a machine learning model. The search for good parameters\n",
            "is known as optimization and the tool we use is known as an optimizer. For a long time the atom optimizer has\n",
            "been the default choice. But now theres a new exciting challenger the muon optimizer. The muon optimizer is\n",
            "getting increasingly more attention in the machine learning community. Its delivering impressive results on\n",
            "small language models and is about twice as computationally efficient as AdamW. In other words you can train\n",
            "faster use less memory and still get great results. Lets first revisit Adam. In standard supervised learning\n",
            "we have a model that makes predictions based on the input data. At first these predictions are just random\n",
            "guesses. Since the models parameters are initialized randomly we use the training data to compute the gradient\n",
            "of the loss with respect to each parameter. The gradient acts like a guide showing us which direction the\n",
            "parameters should move to reduce the loss most effectively. By updating the parameters in this direction at\n",
            "each step the model gradually becomes better at making accurate predictions. This is known as gradient\n",
            "descent. Adam builds on gradient descent but maintains two exponential moving averages of variables. One for\n",
            "the past gradients themselves called momentum and another for the square gradients. Rather than updating\n",
            "parameters directly from the raw gradients Adam combines the momentum with an adaptive scaling factor derived\n",
            "from the square gradients. This allows Adam to converge more quickly and often achieve better results than\n",
            "standard gradient descent. But theres a catch. Adam requires keeping two extra variables for every model\n",
            "parameter. As a result the optimizer states takes up about twice as much memory as the model itself.\n",
            "Furthermore Adam treats all the parameters as a single long vector updating each value independently without\n",
            "considering any internal structure. This approach is called a vector-based optimizer. But can we explicitly\n",
            "account for the underlying matrix structure of the model parameters? Linear layers are especially common in\n",
            "neural networks. Here a linear layer transform the input vector X into an output vector Z. Each output Z is\n",
            "just a weighted sum of the input X where the weights themselves are the trainable parameters. We can describe\n",
            "this input output relationship concisely using a matrix vector product. To update the weights we first\n",
            "calculate the momentum for each parameter. But with vector-based atomizers like atom the momentum for a linear\n",
            "layer naturally a 2D matrix tends to become almost row rank in practice. This means that only a small number\n",
            "of dominant directions really drive the updates while many other directions contribute very little. The muon\n",
            "optimizer tackles this issue by osogonalizing the momentum matrix. By doing so muong amplifies the effect of\n",
            "rare directions the directions that typically receive a small or infrequent updates. Even though these rare\n",
            "dractions seems minor they are often essential for effective learning and can help capture more nuance pattern\n",
            "in the data. Lets get more concrete. Suppose we have a 2D momentum matrix which we call M. Autoonization is\n",
            "the process of finding a new matrix O that is as close as possible to M but with the special property that its\n",
            "rows and columns are all oxonal to each other. A key property of orthogonal matrixes is that their transpose\n",
            "is also their inverse. To build some intuition imagine a momentum matrix as a single point in the\n",
            "highdimensional space of all possible matrices. Our objective is to find the nearest matrix O to M that\n",
            "satisfies the orthogonality condition. This sounds hard. Luckily we have a powerful method for this singular\n",
            "value decomposition. Lets make this more intuitive with a concrete example. A 2D matrix M defines the linear\n",
            "transformation. Think of applying the matrix M to the standard basis vectors. When we multiply M by 1 0 we get\n",
            "2 0. This is just the first column of the matrix M. Next if we multiply M by 01 we get 1.5 1.0 which is the\n",
            "second column. In a sense a 2D matrix records how the transformation moves the basics vectors to new position\n",
            "in space. These four numbers fully determine how we transform any input 2D vectors. Remarkably any linear\n",
            "transformation can be broken down into three steps. A rotation followed by a stretching or shrinking along\n",
            "each axis and then another rotation. Mathematically this process is called singular value decomposition or\n",
            "SVD. It allows us to express any 2D matrix as the product of three special matrices U S and VRpose. When we\n",
            "apply a linear transformation to a vector we can think of it as first rotating by VRpose then scaling each\n",
            "coordinate by the diagonal entry in S and finally rotating again by U. Both U and B are normal matrices which\n",
            "means that their rows and columns are mutually oxonum and have unit lengths. This means that we can use SVD to\n",
            "tackle the oonization problem. By computing the SVD of our momentum matrix then setting all the singular\n",
            "values in S to one. We obtain the osono matrix we want. Easy right? But performing SVD on a matrix is\n",
            "computationally intensive. We cannot afford running this step for every update iterations when training our\n",
            "model. Luckily theres an efficient alternative. We can use something called an odd polomial matrix. This\n",
            "function takes a matrix X as input and computes the weighted sum of X and X XRpose * X. But why might this be\n",
            "useful? Lets unpack how this helps with our 2D matrix example. First we can rewrite the equation by factoring\n",
            "out the matrix M. Then we substitute for M using its SVD form. Notice that the product between V and VRpose\n",
            "equals the identity matrix since V is also normal. Because S is a diagonal matrix multiplying it by itself\n",
            "just square each of its diagonal entries. As we distributed the matrix multiplications certain terms like\n",
            "uranspose U simplify to the identity leading to a much cleaner expression. In the end we see that the left\n",
            "side of the equation has the matrix U while the right side has the matrix VRpose. As a result we can combine\n",
            "the terms to further simplify the expression. This says that applying an R polomial matrix function to M acts\n",
            "on its singular values in the same way as applying the function to each singular value individually. Then\n",
            "reconstructing the matrix with the original singular vectors. This principles applies to any R polomial\n",
            "including higher order values like a fifth order polomial. By choosing appropriate values for the coefficient\n",
            "A B and C we can push the singular values closer to one or without explicitly computing the SVD. Suppose we\n",
            "set A to 1.5 B to minus0.5 and C to zero. We can visualize the effect of this function on a singular value by\n",
            "plotting its input output relationship. The recorder represents how an input value x is mapped to the output\n",
            "value y. Well focus on the input range between zero and one. Since our singular values will fall within this\n",
            "interval our goal here is to turn any input values to an output value that is closer to one. To visualize this\n",
            "we plot the yellow dots evenly spaced along the x-axis between zero and one. After applying the function\n",
            "notice how these yellow dots are moved toward one on the y-axis. This is great. Lets see what happens when we\n",
            "apply this function repeatedly. Each time we apply the function all points are pulled closer to one. In the\n",
            "plots below we can see how the functions behaves over several iterations. After five iteration almost all\n",
            "input values end up very close to one. Now lets try changing the coefficients to see how this affects the\n",
            "transformation. For example we might say a = to 2 b= to minus 1.5 and c equals to 0.5. Here the red curve\n",
            "shows the new r polomial function. By plotting the function after each iteration we can see that the values\n",
            "converge to one even more quickly with these coefficients. But is this the best we can do? Lets tune the value\n",
            "of a b c so that we can get even faster convergence. First increasing the value of a is crucial as this\n",
            "coefficient primary control how quickly small initial singular values converge towards one. Second it turns\n",
            "out empirically we dont have to make the singular values converge to one exactly. We just need them to be\n",
            "bounded by a certain range. for example between 0.7 and 1.3. This leads to ununed coefficients of A B and C.\n",
            "After just a few iterations we can map any singular values between zero and one to the desired range. With\n",
            "this trick we can now write down the algorithm. For each update iteration we first compute the gradient GT\n",
            "using B propagation. We then update the momentum as an exponential moving average of the past gradients. Next\n",
            "we normalize the 2D momentum matrix so that it has uniform. This ensures that the initial singular values are\n",
            "all between zero and one. We repeat this ochronization process five times to get matrix O and then use O to\n",
            "update the parameters. Each iteration involves only matrix multiplications which can be efficiently computed\n",
            "with GPUs without the need to compute SVD. This method is called momentum or synchronization by Newton shorts\n",
            "or muong. But when scaling up to train a larger model the performance gains over AdamW diminish. To resolve\n",
            "this issue we add the weight decay mechanism as used in Atom W. In addition we adjust the learning rate by\n",
            "taking account the size of the 2D matrix. The two improvements help stabilize the trending of large models.\n",
            "But theres still a challenge. Researchers have observed that as trending continues the attention logics can\n",
            "grow larger and larger which may cause the trending process to become unstable. Where does that come from and\n",
            "how can we fix it? Consider a simple scenario. Suppose we have a sequence of four tokens. Each token is mapped\n",
            "to an embedding vector of dimension D. Lets call the matrix of these embeddings X. For simplicity well focus\n",
            "on self attention in the first transformer block. Although the situation is similar in all layers we obtain\n",
            "the query key and value representations by projecting the input embedding X with the parameter matrix WQ WK\n",
            "and WV. Next the attention mechanism computes a weighted sum of the value vectors where the weights are\n",
            "determined by the attention scores. The attention logics before the softmax are computed by multiplying the\n",
            "query matrix Q with the transpose of the key matrix K. Here we substitute the expressions of Q and K into the\n",
            "formula and further simplify the attention calculation. Note that the matrix X and its transpose denotes the\n",
            "embedding vectors which are typically normalized to have unit norms. To prevent the attention logics from\n",
            "becoming excessively large we must control the scale of WQ and WK. A common strategy is to apply a scaling\n",
            "vector to these matrixes. During training we monitor the maximum value of the attention logics. If it exists a\n",
            "certain threshold towel we calculate a scaling ratio denoted as gamma. The idea is simple. When the attention\n",
            "logics surpass the threshold towel we simply scales the relevant model parameters by the factor gamma to keep\n",
            "them in check. Because both WQ and WK contribute to the attention logics we scale each of these matrix by the\n",
            "square root of gamma. The revised algorithm looks like this. First we update the model parameter SA using\n",
            "muong optimizer. Next if the maximum attention logics is larger than toao we rescale both WQ and WK by\n",
            "multiplying them with the square root of gamma. This trick is called QK clip. By doing so we directly\n",
            "constrain attention logics ensuring that they stay within a safe range by rescaling the query and key\n",
            "projection weights. This looks great but in practice self attention consists of multiple hands. We achieve\n",
            "this by splitting the query key and value matrix into several hands four in our example. For each hand we\n",
            "regroup Q K and V. compute attention independently and concatenate the outputs from all hands and project them\n",
            "with a output matrix WO. When the maximal attention logics go beyond the threshold it does not make sense to\n",
            "rescale all the hands in the same way. Instead we introduce an individual scaling factor for each hand to\n",
            "control their logics separately. But things get tricky if we want to use multi-head latent attention MLA\n",
            "proposed by DC. MLA compress the query key and value representations into a low rank space to reduce the size\n",
            "of the KB cache. This compression is performed using a D projection matrix which produce latent\n",
            "representations. These compressed latent vectors are then mapped back to the query key and values for each\n",
            "attention hands using the corresponding R projection matrices. But a challenge will arise because this low\n",
            "rank key value compression does not work with rotary position embedding. To overcome this limitation\n",
            "researcher proposed a decoupled rope technique which introduce extra multi head queries and a share key to\n",
            "encode positional information. For MOA the query key and values are regrouped for each head. Specifically for\n",
            "the query we concatenate the compressed query component QC with the rotated query QR. Similarly the key is\n",
            "constructed by concatenating the compressed key KC with its rotated counterpart KR. When using multi-head\n",
            "latent tension we need to carefully decide how to rescale these four matrixes. For the R projection matrix we\n",
            "rescale the parameters for each hat individually. The row component deserves special attention. In this setup\n",
            "each hat has its own rotary query WQR but all hats share a single rotary key matrix WKR. If we were to apply\n",
            "the same perhaps scaling for both the sure WKR matrix will be rescaled multiple times which is undesirable. To\n",
            "handle this properly we rescale only the has specific rotary queries WQR by their respective gamma H while\n",
            "leaving the share rotary key matrix WKR unchanged. This technique is called muon clip. Lets compare training\n",
            "with and with our muon clip. With muon clip applied as shown on the right the maximum attention logics are\n",
            "effectively kept and quickly stabilized demonstrating the effectiveness of QK clip regulation. This helps the\n",
            "optimizer maintain steady and reliable training. I hope you enjoy this overview. Thanks for watching and Ill\n",
            "see you next\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "from typing import List, Optional\n",
        "\n",
        "def _normalize_token(t: str) -> str:\n",
        "    # remove angle-tags, lower-case, strip surrounding punctuation\n",
        "    t = re.sub(r\"<.*?>\", \"\", t)\n",
        "    t = t.lower()\n",
        "    t = re.sub(r\"^[^\\w']+|[^\\w']+$\", \"\", t)  # strip leading/trailing punctuation (keep apostrophes)\n",
        "    return t\n",
        "\n",
        "def remove_consecutive_duplicate_phrases(words: List[str],\n",
        "                                        max_seq_len: Optional[int] = None,\n",
        "                                        normalize: bool = False) -> List[str]:\n",
        "    \"\"\"\n",
        "    Collapse consecutive duplicated phrases in a token list.\n",
        "      - words: list of word tokens (strings)\n",
        "      - max_seq_len: maximum phrase length to consider (None -> n//2)\n",
        "      - normalize: if True, compare using a normalized version of tokens (lowercase, strip tags/punct)\n",
        "    Returns a cleaned list of tokens with duplicates collapsed (one copy kept).\n",
        "    \"\"\"\n",
        "    n = len(words)\n",
        "    if n == 0:\n",
        "        return []\n",
        "\n",
        "    if max_seq_len is None:\n",
        "        max_seq_len = n // 2\n",
        "    max_seq_len = max(1, min(max_seq_len, n // 2))\n",
        "\n",
        "    # optionally build normalized view for comparisons\n",
        "    if normalize:\n",
        "        norm_words = [_normalize_token(w) for w in words]\n",
        "    else:\n",
        "        norm_words = words\n",
        "\n",
        "    cleaned: List[str] = []\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        found = False\n",
        "        # largest possible chunk at i that can repeat at least once\n",
        "        max_L = min(max_seq_len, (n - i) // 2)\n",
        "        # try long -> short so we prefer whole-sentence matches\n",
        "        for L in range(max_L, 0, -1):\n",
        "            a_norm = norm_words[i:i+L]\n",
        "            b_norm = norm_words[i+L:i+2*L]\n",
        "            if a_norm == b_norm:\n",
        "                # count how many consecutive copies of this chunk exist\n",
        "                count = 1\n",
        "                while i + (count+1)*L <= n and norm_words[i + count*L : i + (count+1)*L] == a_norm:\n",
        "                    count += 1\n",
        "                # keep ONE copy (the original tokens, not normalized)\n",
        "                cleaned.extend(words[i:i+L])\n",
        "                # skip all copies\n",
        "                i += count * L\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            # no repeated chunk starting at i -> keep single token\n",
        "            cleaned.append(words[i])\n",
        "            i += 1\n",
        "\n",
        "    return str(cleaned)\n",
        "\n",
        "# ----- quick tests -----\n",
        "long_text = remove_consecutive_duplicate_phrases(text)  # your video transcript\n",
        "\n",
        "# Wrap text to 80 characters per line\n",
        "joined_text = ''\n",
        "for word in long_text:\n",
        "  joined_text += word\n",
        "\n",
        "replace = ['\"',\"'\",\",\"]\n",
        "for punc in replace:\n",
        "  joined_text = joined_text.replace(punc,\"\")\n",
        "new_text = joined_text.strip(\"[]\")\n",
        "lister = new_text.split()\n",
        "newer_text = lister[5:-1]\n",
        "newest_text = \" \".join(newer_text)\n",
        "\n",
        "wrapped_text = textwrap.fill(newest_text, width=110)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The muon optimizer is getting increasingly more attention in the machine learning community. Its delivering\n",
            "impressive results on small language models and is about twice as computationally efficient as AdamW.\n",
            "Mathematically this process is called singular value decomposition or SVD. It allows us to express any 2D\n",
            "matrix as the product of three special matrices U S and VRpose. By computing the SVD of our momentum matrix\n",
            "then setting all the singular values in S to one we obtain the osono matrix we want. Researchers have observed\n",
            "that as trending continues the attention logics can grow larger and larger. This may cause the trending\n",
            "process to become unstable. To resolve this issue we add the weight decay mechanism as used in Atom W. We\n",
            "adjust the learning rate by taking account the size of the 2D matrix. Multi-head latent attention MLA proposed\n",
            "by DC. MLA compress the query key and value representations into a low rank space to reduce the size of the KB\n",
            "cache. But this low rank key value compression does not work with rotary position embedding. To overcome this\n",
            "limitation researcher proposed a decoupled rope technique which introduce extra multi head queries and a share\n",
            "key.\n"
          ]
        }
      ],
      "source": [
        "# Our text is too long! We gotta chunk it up\n",
        "# The model has a max token limit (1024 for BART)\n",
        "# 1 token about 0.75 words on average\n",
        "# So let's chunk text into 500 word chunks just for safety\n",
        "def chunk_text(text, word_limit = 700):\n",
        "    text = text.split()\n",
        "    text_length = len(text)\n",
        "    chunks = []\n",
        "    num_chunks = (text_length // word_limit) + 1 \n",
        "    for i in range(num_chunks):  \n",
        "        if i == num_chunks - 1: # last chunk\n",
        "            # join to the end\n",
        "            chunks.append(\" \".join(text[i*word_limit:]))\n",
        "        else:\n",
        "            # join for every 500 chunks\n",
        "            chunks.append(\" \".join(text[i*word_limit:(i+1)*word_limit]))\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(newest_text)\n",
        "overall_summary = \"\"\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "for chunk in chunks:\n",
        "    # Now you can use the summarizer\n",
        "    summary = summarizer(chunk, max_length=250, min_length=30, do_sample=False)\n",
        "    overall_summary += summary[0]['summary_text'] + \" \"\n",
        "wrapped_summary = textwrap.fill(overall_summary, width = 110)\n",
        "print(wrapped_summary)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
