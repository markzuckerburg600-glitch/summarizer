{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XumW1PvfB-36"
      },
      "outputs": [],
      "source": [
        "import subprocess #lets us download captions\n",
        "import os\n",
        "import re # Detect lines that are just numbers and skip them\n",
        "import glob #let's python search for files matching a pattern\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JPiO_cZzCCqg"
      },
      "outputs": [],
      "source": [
        "video_url = \"https://www.youtube.com/watch?v=bO5nvE289ec\"\n",
        "# def url_spliter(video_url):\n",
        "#   id1 = video_url.split(\"/watch?v=\")[-1]\n",
        "#   id2 = id1.split(\"&\")\n",
        "#   id3 = id2[0]\n",
        "#   return id3\n",
        "\n",
        "#One way to get video URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6JvLLfmVCk9r"
      },
      "outputs": [],
      "source": [
        "import subprocess #lets us download captions\n",
        "import os\n",
        "import re # Detect lines that are just numbers and skip them\n",
        "import glob #let's python search for files matching a pattern\n",
        "\n",
        "def get_transcript(video_url: str, lang = \"en\", cookies_file = \"cookies (1).txt\") -> str:\n",
        "  existing_vtts = set(glob.glob(\"*.vtt\")) # Stores videos in a set to see which video was downloaded\n",
        "\n",
        "  # First, get information about available captions\n",
        "  info_result = subprocess.run([\n",
        "      \"yt-dlp\",\n",
        "      \"--list-subs\",\n",
        "      video_url\n",
        "  ], check=False, text=True, capture_output=True)\n",
        "\n",
        "  print(\"Available captions:\")\n",
        "  print(info_result.stdout)\n",
        "  print(info_result.stderr)\n",
        "\n",
        "\n",
        "  result = subprocess.run([\n",
        "  \"yt-dlp\", #download captions\n",
        "   \"--write-auto-sub\", #auto subtitles if possible\n",
        "  \"--write-sub\", #Also can use creator captions if they have it\n",
        "  \"--sub-langs\", f\"{lang}\", #english language\n",
        "  \"--skip-download\", #prevents actual video from being downloaded\n",
        "  \"--output\", \"%(id)s.%(ext)s\", #forces file names to be VIDEOID.ext\n",
        "  \"--verbose\", # Add verbose flag for debugging\n",
        "  video_url #enter actual link\n",
        "  ], check = False, text = True, capture_output = True)\n",
        "\n",
        "\n",
        "# This creates a new folder that's not in the existing_vtt with the captions\n",
        "\n",
        "# new_vtts is a list. glob.glob searches files that has the vtt ending.\n",
        "# For every file with the ending, if it's not in our existing_vtt, then we know\n",
        "# it has to be a caption file, so we put it in new_vtt.\n",
        "  new_vtts = [f for f in glob.glob(\"*.vtt\") if f not in existing_vtts]\n",
        "\n",
        "# If the caption file doesn't work and nothing is new, raise error\n",
        "  if not new_vtts:\n",
        "    print(\"No creator captions, trying auto captions\")\n",
        "    other_result = subprocess.run([\n",
        "    \"yt-dlp\", #download captions\n",
        "    \"--write-auto-sub\", #auto subtitles if possible\n",
        "    \"--write-sub\", #Also can use creator captions if they have it\n",
        "    \"--sub-langs\", f\"{lang}\", #english language\n",
        "    \"--skip-download\", #prevents actual video from being downloaded\n",
        "    \"--output\", \"%(id)s.%(ext)s\", #forces file names to be VIDEOID.ext\n",
        "    \"--verbose\", # Add verbose flag for debugging\n",
        "    video_url #enter actual link\n",
        "    ], check = False, text = True, capture_output = True)\n",
        "\n",
        "  new_vtts = [f for f in glob.glob(\"*.vtt\") if f not in existing_vtts]\n",
        "\n",
        "  if not new_vtts:\n",
        "    raise FileNotFoundError(\"Caption file not found. Maybe captions aren't available for this video.\")\n",
        "\n",
        "# Get rid of empty spaces and arrows\n",
        "  vtt_file = new_vtts[0]\n",
        "  print(f\"Using caption file {vtt_file}\")\n",
        "  transcript = \"\"\n",
        "  with open(vtt_file, \"r\", encoding = \"utf-8\") as f:\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      if not line:\n",
        "        continue\n",
        "      if \"-->\" in line:\n",
        "        continue\n",
        "      if re.match(r\"^\\d+$\", line):\n",
        "        continue\n",
        "      transcript += line + \" \"\n",
        "    cleaned_transcript = re.sub(r\"<.*?>\", \" \", transcript)\n",
        "  return cleaned_transcript.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "3RX4ilwdVJts",
        "outputId": "5d8f13b6-bcf9-451a-8a54-928bfd492b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available captions:\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=bO5nvE289ec\n",
            "[youtube] bO5nvE289ec: Downloading webpage\n",
            "[youtube] bO5nvE289ec: Downloading tv client config\n",
            "[youtube] bO5nvE289ec: Downloading tv player API JSON\n",
            "[youtube] bO5nvE289ec: Downloading web safari player API JSON\n",
            "[youtube] bO5nvE289ec: Downloading player 0004de42-main\n",
            "[youtube] bO5nvE289ec: Downloading m3u8 information\n",
            "[info] Available automatic captions for bO5nvE289ec:\n",
            "Language      Name                                               Formats\n",
            "en-US                                                            vtt\n",
            "ab            Abkhazian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "aa            Afar                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "af            Afrikaans                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ak            Akan                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sq            Albanian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "am            Amharic                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ar            Arabic                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hy            Armenian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "as            Assamese                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ay            Aymara                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "az            Azerbaijani                                        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bn            Bangla                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ba            Bashkir                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eu            Basque                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "be            Belarusian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bho           Bhojpuri                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bs            Bosnian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "br            Breton                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bg            Bulgarian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "my            Burmese                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ca            Catalan                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ceb           Cebuano                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hans       Chinese (Simplified)                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hant       Chinese (Traditional)                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "co            Corsican                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hr            Croatian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cs            Czech                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "da            Danish                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dv            Divehi                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nl            Dutch                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dz            Dzongkha                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "en-orig       English (Original)                                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "en            English                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eo            Esperanto                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "et            Estonian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ee            Ewe                                                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fo            Faroese                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fj            Fijian                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fil           Filipino                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fi            Finnish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fr            French                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gaa           Ga                                                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gl            Galician                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lg            Ganda                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ka            Georgian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "de            German                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "el            Greek                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gn            Guarani                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gu            Gujarati                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ht            Haitian Creole                                     vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ha            Hausa                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "haw           Hawaiian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iw            Hebrew                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hi            Hindi                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hmn           Hmong                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hu            Hungarian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "is            Icelandic                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ig            Igbo                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "id            Indonesian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iu            Inuktitut                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ga            Irish                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "it            Italian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ja            Japanese                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "jv            Javanese                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kl            Kalaallisut                                        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kn            Kannada                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kk            Kazakh                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kha           Khasi                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "km            Khmer                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rw            Kinyarwanda                                        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ko            Korean                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kri           Krio                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ku            Kurdish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ky            Kyrgyz                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lo            Lao                                                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "la            Latin                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lv            Latvian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ln            Lingala                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lt            Lithuanian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lua           Luba-Lulua                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "luo           Luo                                                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lb            Luxembourgish                                      vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mk            Macedonian                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mg            Malagasy                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ms            Malay                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ml            Malayalam                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mt            Maltese                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gv            Manx                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mi            Māori                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mr            Marathi                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mn            Mongolian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mfe           Morisyen                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ne            Nepali                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "new           Newari                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nso           Northern Sotho                                     vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "no            Norwegian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ny            Nyanja                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "oc            Occitan                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "or            Odia                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "om            Oromo                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "os            Ossetic                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pam           Pampanga                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ps            Pashto                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fa            Persian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pl            Polish                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt            Portuguese                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt-PT         Portuguese (Portugal)                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pa            Punjabi                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "qu            Quechua                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ro            Romanian                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rn            Rundi                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ru            Russian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sm            Samoan                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sg            Sango                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sa            Sanskrit                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gd            Scottish Gaelic                                    vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sr            Serbian                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "crs           Seselwa Creole French                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sn            Shona                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sd            Sindhi                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "si            Sinhala                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sk            Slovak                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sl            Slovenian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "so            Somali                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "st            Southern Sotho                                     vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "es            Spanish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "su            Sundanese                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sw            Swahili                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ss            Swati                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sv            Swedish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tg            Tajik                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ta            Tamil                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tt            Tatar                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "te            Telugu                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "th            Thai                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bo            Tibetan                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ti            Tigrinya                                           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "to            Tongan                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ts            Tsonga                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tn            Tswana                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tum           Tumbuka                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tr            Turkish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tk            Turkmen                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uk            Ukrainian                                          vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ur            Urdu                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ug            Uyghur                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uz            Uzbek                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ve            Venda                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "vi            Vietnamese                                         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "war           Waray                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cy            Welsh                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fy            Western Frisian                                    vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "wo            Wolof                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "xh            Xhosa                                              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yi            Yiddish                                            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yo            Yoruba                                             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zu            Zulu                                               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ab-en-US      Abkhazian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "aa-en-US      Afar from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "af-en-US      Afrikaans from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ak-en-US      Akan from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sq-en-US      Albanian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "am-en-US      Amharic from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ar-en-US      Arabic from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hy-en-US      Armenian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "as-en-US      Assamese from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ay-en-US      Aymara from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "az-en-US      Azerbaijani from English (United States)           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bn-en-US      Bangla from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ba-en-US      Bashkir from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eu-en-US      Basque from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "be-en-US      Belarusian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bho-en-US     Bhojpuri from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bs-en-US      Bosnian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "br-en-US      Breton from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bg-en-US      Bulgarian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "my-en-US      Burmese from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ca-en-US      Catalan from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ceb-en-US     Cebuano from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hans-en-US Chinese (Simplified) from English (United States)  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zh-Hant-en-US Chinese (Traditional) from English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "co-en-US      Corsican from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hr-en-US      Croatian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cs-en-US      Czech from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "da-en-US      Danish from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dv-en-US      Divehi from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nl-en-US      Dutch from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "dz-en-US      Dzongkha from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "en-en-US      English from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "eo-en-US      Esperanto from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "et-en-US      Estonian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ee-en-US      Ewe from English (United States)                   vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fo-en-US      Faroese from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fj-en-US      Fijian from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fil-en-US     Filipino from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fi-en-US      Finnish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fr-en-US      French from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gaa-en-US     Ga from English (United States)                    vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gl-en-US      Galician from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lg-en-US      Ganda from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ka-en-US      Georgian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "de-en-US      German from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "el-en-US      Greek from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gn-en-US      Guarani from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gu-en-US      Gujarati from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ht-en-US      Haitian Creole from English (United States)        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ha-en-US      Hausa from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "haw-en-US     Hawaiian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iw-en-US      Hebrew from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hi-en-US      Hindi from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hmn-en-US     Hmong from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "hu-en-US      Hungarian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "is-en-US      Icelandic from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ig-en-US      Igbo from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "id-en-US      Indonesian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "iu-en-US      Inuktitut from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ga-en-US      Irish from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "it-en-US      Italian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ja-en-US      Japanese from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "jv-en-US      Javanese from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kl-en-US      Kalaallisut from English (United States)           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kn-en-US      Kannada from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kk-en-US      Kazakh from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kha-en-US     Khasi from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "km-en-US      Khmer from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rw-en-US      Kinyarwanda from English (United States)           vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ko-en-US      Korean from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "kri-en-US     Krio from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ku-en-US      Kurdish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ky-en-US      Kyrgyz from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lo-en-US      Lao from English (United States)                   vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "la-en-US      Latin from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lv-en-US      Latvian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ln-en-US      Lingala from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lt-en-US      Lithuanian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lua-en-US     Luba-Lulua from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "luo-en-US     Luo from English (United States)                   vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "lb-en-US      Luxembourgish from English (United States)         vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mk-en-US      Macedonian from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mg-en-US      Malagasy from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ms-en-US      Malay from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ml-en-US      Malayalam from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mt-en-US      Maltese from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gv-en-US      Manx from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mi-en-US      Māori from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mr-en-US      Marathi from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mn-en-US      Mongolian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "mfe-en-US     Morisyen from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ne-en-US      Nepali from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "new-en-US     Newari from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "nso-en-US     Northern Sotho from English (United States)        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "no-en-US      Norwegian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ny-en-US      Nyanja from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "oc-en-US      Occitan from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "or-en-US      Odia from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "om-en-US      Oromo from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "os-en-US      Ossetic from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pam-en-US     Pampanga from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ps-en-US      Pashto from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fa-en-US      Persian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pl-en-US      Polish from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt-en-US      Portuguese from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pt-PT-en-US   Portuguese (Portugal) from English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "pa-en-US      Punjabi from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "qu-en-US      Quechua from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ro-en-US      Romanian from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "rn-en-US      Rundi from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ru-en-US      Russian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sm-en-US      Samoan from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sg-en-US      Sango from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sa-en-US      Sanskrit from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "gd-en-US      Scottish Gaelic from English (United States)       vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sr-en-US      Serbian from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "crs-en-US     Seselwa Creole French from English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sn-en-US      Shona from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sd-en-US      Sindhi from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "si-en-US      Sinhala from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sk-en-US      Slovak from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sl-en-US      Slovenian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "so-en-US      Somali from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "st-en-US      Southern Sotho from English (United States)        vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "es-en-US      Spanish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "su-en-US      Sundanese from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sw-en-US      Swahili from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ss-en-US      Swati from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "sv-en-US      Swedish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tg-en-US      Tajik from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ta-en-US      Tamil from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tt-en-US      Tatar from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "te-en-US      Telugu from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "th-en-US      Thai from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "bo-en-US      Tibetan from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ti-en-US      Tigrinya from English (United States)              vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "to-en-US      Tongan from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ts-en-US      Tsonga from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tn-en-US      Tswana from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tum-en-US     Tumbuka from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tr-en-US      Turkish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "tk-en-US      Turkmen from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uk-en-US      Ukrainian from English (United States)             vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ur-en-US      Urdu from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ug-en-US      Uyghur from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "uz-en-US      Uzbek from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "ve-en-US      Venda from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "vi-en-US      Vietnamese from English (United States)            vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "war-en-US     Waray from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "cy-en-US      Welsh from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "fy-en-US      Western Frisian from English (United States)       vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "wo-en-US      Wolof from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "xh-en-US      Xhosa from English (United States)                 vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yi-en-US      Yiddish from English (United States)               vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "yo-en-US      Yoruba from English (United States)                vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "zu-en-US      Zulu from English (United States)                  vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "[info] Available subtitles for bO5nvE289ec:\n",
            "Language Name                    Formats\n",
            "en-US    English (United States) vtt, srt, ttml, srv3, srv2, srv1, json3\n",
            "\n",
            "\n",
            "Using caption file bO5nvE289ec.en.vtt\n",
            "['WEBVTT', 'Kind:', 'captions', 'Language:', 'en', 'This', 'video', 'is', 'about', 'how', 'to', 'find', 'good', 'This', 'video', 'is', 'about', 'how', 'to', 'find', 'good', 'This', 'video', 'is', 'about', 'how', 'to', 'find', 'good', 'parameters', 'for', 'a', 'machine', 'learning', 'model.', 'parameters', 'for', 'a', 'machine', 'learning', 'model.', 'parameters', 'for', 'a', 'machine', 'learning', 'model.', 'The', 'search', 'for', 'good', 'parameters', 'is', 'known', 'The', 'search', 'for', 'good', 'parameters', 'is', 'known', 'The', 'search', 'for', 'good', 'parameters', 'is', 'known', 'as', 'optimization', 'and', 'the', 'tool', 'we', 'use', 'is', 'as', 'optimization', 'and', 'the', 'tool', 'we', 'use', 'is', 'as', 'optimization', 'and', 'the', 'tool', 'we', 'use', 'is', 'known', 'as', 'an', 'optimizer.', 'For', 'a', 'long', 'time,']\n"
          ]
        }
      ],
      "source": [
        "text = get_transcript(video_url)\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5eGXZylbPzT",
        "outputId": "324924e3-fef6-4e59-ebde-ccce7bee3a0f"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from typing import List, Optional\n",
        "\n",
        "def _normalize_token(t: str) -> str:\n",
        "    # remove angle-tags, lower-case, strip surrounding punctuation\n",
        "    t = re.sub(r\"<.*?>\", \"\", t)\n",
        "    t = t.lower()\n",
        "    t = re.sub(r\"^[^\\w']+|[^\\w']+$\", \"\", t)  # strip leading/trailing punctuation (keep apostrophes)\n",
        "    return t\n",
        "\n",
        "def remove_consecutive_duplicate_phrases(words: List[str],\n",
        "                                        max_seq_len: Optional[int] = None,\n",
        "                                        normalize: bool = False) -> List[str]:\n",
        "    \"\"\"\n",
        "    Collapse consecutive duplicated phrases in a token list.\n",
        "      - words: list of word tokens (strings)\n",
        "      - max_seq_len: maximum phrase length to consider (None -> n//2)\n",
        "      - normalize: if True, compare using a normalized version of tokens (lowercase, strip tags/punct)\n",
        "    Returns a cleaned list of tokens with duplicates collapsed (one copy kept).\n",
        "    \"\"\"\n",
        "    n = len(words)\n",
        "    if n == 0:\n",
        "        return []\n",
        "\n",
        "    if max_seq_len is None:\n",
        "        max_seq_len = n // 2\n",
        "    max_seq_len = max(1, min(max_seq_len, n // 2))\n",
        "\n",
        "    # optionally build normalized view for comparisons\n",
        "    if normalize:\n",
        "        norm_words = [_normalize_token(w) for w in words]\n",
        "    else:\n",
        "        norm_words = words\n",
        "\n",
        "    cleaned: List[str] = []\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        found = False\n",
        "        # largest possible chunk at i that can repeat at least once\n",
        "        max_L = min(max_seq_len, (n - i) // 2)\n",
        "        # try long -> short so we prefer whole-sentence matches\n",
        "        for L in range(max_L, 0, -1):\n",
        "            a_norm = norm_words[i:i+L]\n",
        "            b_norm = norm_words[i+L:i+2*L]\n",
        "            if a_norm == b_norm:\n",
        "                # count how many consecutive copies of this chunk exist\n",
        "                count = 1\n",
        "                while i + (count+1)*L <= n and norm_words[i + count*L : i + (count+1)*L] == a_norm:\n",
        "                    count += 1\n",
        "                # keep ONE copy (the original tokens, not normalized)\n",
        "                cleaned.extend(words[i:i+L])\n",
        "                # skip all copies\n",
        "                i += count * L\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            # no repeated chunk starting at i -> keep single token\n",
        "            cleaned.append(words[i])\n",
        "            i += 1\n",
        "\n",
        "    return str(cleaned)\n",
        "\n",
        "# ----- quick tests -----\n",
        "long_text = remove_consecutive_duplicate_phrases(text)  # your video transcript\n",
        "\n",
        "# Wrap text to 80 characters per line\n",
        "joined_text = ''\n",
        "for word in long_text:\n",
        "  joined_text += word\n",
        "\n",
        "replace = ['\"',\"'\",\",\"]\n",
        "for punc in replace:\n",
        "  joined_text = joined_text.replace(punc,\"\")\n",
        "new_text = joined_text.strip(\"[]\")\n",
        "lister = new_text.split()\n",
        "newer_text = lister[5:-1]\n",
        "newest_text = \" \".join(newer_text)\n",
        "\n",
        "wrapped_text = textwrap.fill(newest_text, width=110)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PGEdPHuTfn1z",
        "outputId": "500c4acf-e544-426b-a604-ba69c8e3c55d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.8.0+cpu\n",
            "Uninstalling torch-2.8.0+cpu:\n",
            "  Successfully uninstalled torch-2.8.0+cpu\n",
            "Found existing installation: torchvision 0.23.0+cpu\n",
            "Uninstalling torchvision-0.23.0+cpu:\n",
            "  Successfully uninstalled torchvision-0.23.0+cpu\n",
            "Found existing installation: torchaudio 2.8.0+cpu\n",
            "Uninstalling torchaudio-2.8.0+cpu:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cpu\n",
            "Found existing installation: transformers 4.55.3\n",
            "Uninstalling transformers-4.55.3:\n",
            "  Successfully uninstalled transformers-4.55.3\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (183.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "Using cached https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "peft 0.17.0 requires transformers, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.8.0+cpu torchaudio-2.8.0+cpu torchvision-0.23.0+cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "9ad9779e2be34ceda0275c5761557999",
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Using cached transformers-4.55.3-py3-none-any.whl (11.3 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.55.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "fd228091d437435e82beb114f2d42466",
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# !pip uninstall -y torch torchvision torchaudio transformers\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "# !pip install transformers\n",
        "# from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR5R8BTkuHcs",
        "outputId": "9c57e204-e939-43ba-c7ba-a3d5cb29942e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# model_name = \"bigscience/bloom-560m\"  # smaller BLOOM for free usage\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# cleaned_text = remove_consecutive_duplicate_phrases(text)\n",
        "\n",
        "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "# summary = generator(\n",
        "#     cleaned_text,\n",
        "#     max_new_tokens = 100,\n",
        "#     do_sample=True,\n",
        "#     temperature=0.7\n",
        "#     )\n",
        "# print(summary[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "ryf9-Jq8mWDk",
        "outputId": "faf15dc7-5553-4dfd-e3a9-d6d9b28e34d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1618 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6715906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0msummary\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-6715906.py\u001b[0m in \u001b[0;36msummarize_transcript\u001b[0;34m(text, max_length, min_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0msummary\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         if (\n\u001b[1;32m    193\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m             )\n\u001b[1;32m   1457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2423\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values_length, position_ids)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2544\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# # Load model\n",
        "# cleaned_text = (remove_consecutive_duplicate_phrases(text))\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "# summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device =-1)\n",
        "\n",
        "# # Split the text into chunks\n",
        "# def chunk_text(text, max_tokens = 900):\n",
        "#   chunks = []\n",
        "#   tokens = tokenizer.encode(text)\n",
        "#   words = str(text).split()\n",
        "#   for chunk in range(0, len(tokens), max_tokens):\n",
        "#     chunks_tokens = tokens[chunk:chunk+max_tokens]\n",
        "#     chunk_text =\n",
        "#   return chunks\n",
        "# chunk_text(cleaned_text)\n",
        "# def summarize_transcript(text: str, max_length=150, min_length=50) -> str:\n",
        "#     chunks = chunk_text(text)\n",
        "#     summary = \"\"\n",
        "#     for chunk in chunks:\n",
        "#       out = summarizer(str(chunk), max_length = 150, min_length=50, do_sample=False)\n",
        "#       summary += out[0]['summary_text'] + \" \"\n",
        "#     return summary\n",
        "# print(summarize_transcript(cleaned_text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
